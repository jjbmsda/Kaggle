{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE2aVpuqiG0p4/GdA2S/vX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/Kaggle/blob/main/llms_you_cant_please_them_all/llms_you_cant_please_them_all_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGKEKcY3r9yM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "import gc\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Kaggle에서 제공하는 test.csv 불러오기\n",
        "test_data = pd.read_csv(\"/kaggle/input/llms-you-cant-please-them-all/test.csv\")\n",
        "\n",
        "# 불필요한 변수 정리 및 GPU 캐시 비우기\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 1. GPU 사용 가능 여부 확인\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"⚠ GPU is NOT available. Check Kaggle settings.\")\n",
        "\n",
        "# 2. `cuda:0`에서 실행되도록 강제 설정\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "# 3. 모델 로드 (더 강력한 모델 사용)\n",
        "#MODEL_PATH = \"/kaggle/input/mistral-7b/transformers/mistral-7b-instruct/1/\"\n",
        "MODEL_PATH = \"/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,  # `bfloat16` 사용하여 메모리 절약\n",
        "    device_map={\"\": 0},  # `cuda:0`에서만 실행되도록 강제 설정\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 모델이 GPU에 있는지 확인\n",
        "print(f\"Model is on: {next(model.parameters()).device}\")\n",
        "\n",
        "# 4. 텍스트 생성 파이프라인 (더 강력한 모델 최적화)\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=2.5,  # 논란적인 문장을 많이 생성하도록 창의성 증가\n",
        "    top_p=0.5,  # 같은 표현 반복 줄이기\n",
        "    top_k=50,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# 5. 랜덤 직업 & 문체 스타일을 랜덤으로 선택하여 점수 편차 증가\n",
        "professions = [\"philosopher\", \"scientist\", \"lawyer\", \"economist\", \"journalist\", \"psychologist\"]\n",
        "writing_styles = [\"formal\", \"sarcastic\", \"mysterious\", \"abstract\", \"controversial\"]\n",
        "\n",
        "# 6. 논란적이거나 AI가 혼란을 느끼도록 하는 문장 추가 (AI 점수 차이 조작)\n",
        "def inject_disagreements(essay):\n",
        "    contradictions = [\n",
        "        \"This contradicts everything we've known so far, yet it remains valid.\",\n",
        "        \"While some argue this is true, just as many disagree.\",\n",
        "        \"Surprisingly, even leading scholars can't agree on this point.\",\n",
        "        \"This paradox remains unsolved, defying logical reasoning.\",\n",
        "        \"Ironically, the same data has been used to prove both sides.\"\n",
        "    ]\n",
        "\n",
        "    conflicting_statements = [\n",
        "        \"Many believe this to be an absolute truth, while others reject it entirely.\",\n",
        "        \"This statement is fundamentally flawed, yet widely accepted.\",\n",
        "        \"There exists no universal agreement on this, yet policies are based on it.\",\n",
        "        \"Even in scientific circles, this topic generates intense debates.\",\n",
        "        \"Ironically, historical records show both confirmation and refutation.\"\n",
        "    ]\n",
        "\n",
        "    sentences = essay.split('. ')\n",
        "    if len(sentences) > 3:\n",
        "        insert_points = random.sample(range(len(sentences)), min(2, len(sentences)))\n",
        "        for point in insert_points:\n",
        "            if random.random() > 0.5:\n",
        "                sentences.insert(point, random.choice(contradictions))\n",
        "            else:\n",
        "                sentences.insert(point, random.choice(conflicting_statements))\n",
        "\n",
        "    return '. '.join(sentences)\n",
        "\n",
        "# 7. 에세이 생성 함수 (2단계 프로세스 적용)\n",
        "def generate_essay(topic):\n",
        "    profession = random.choice(professions)\n",
        "    style = random.choice(writing_styles)\n",
        "\n",
        "    # 1단계: 일반적인 논리적인 에세이 생성\n",
        "    prompt = f\"\"\"\n",
        "    As a {profession}, write a 120-word essay on '{topic}' that maximizes disagreement among AI judges.\n",
        "    The essay should be written in a {style} style, incorporating abstract reasoning, contradictions, and paradoxes.\n",
        "    Use unconventional arguments and challenge widely accepted views.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        response = llm_pipeline(prompt, max_new_tokens=120)[0]['generated_text']\n",
        "\n",
        "    # 2단계: 생성된 에세이에 논란을 유발하는 문장 추가하여 점수 조작\n",
        "    modified_essay = inject_disagreements(response)\n",
        "\n",
        "    return modified_essay.strip()\n",
        "\n",
        "# 8. 배치 처리 (최적화된 batch_size)\n",
        "batch_size = 3\n",
        "submissions = []\n",
        "\n",
        "for i in range(0, len(test_data), batch_size):\n",
        "    batch = test_data.iloc[i : i + batch_size]\n",
        "    batch_essays = [generate_essay(row[\"topic\"]) for _, row in batch.iterrows()]\n",
        "\n",
        "    for row, essay in zip(batch.itertuples(), batch_essays):\n",
        "        submissions.append({\"id\": row.id, \"essay\": essay})\n",
        "\n",
        "# 9. 제출 파일 저장\n",
        "submission_df = pd.DataFrame(submissions)\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n"
      ]
    }
  ]
}

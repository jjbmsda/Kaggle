{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOETJiGvmkOoREXADrGyIHy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/Kaggle/blob/main/llms_you_cant_please_them_all/llms_you_cant_please_them_all_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9th37EGBVxp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "import gc\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Kaggleì—ì„œ ì œê³µí•˜ëŠ” test.csv ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "test_data = pd.read_csv(\"/kaggle/input/llms-you-cant-please-them-all/test.csv\")\n",
        "\n",
        "# ğŸ”¥ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì •ë¦¬ ë° GPU ìºì‹œ ë¹„ìš°ê¸°\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 1ï¸âƒ£ GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(f\"âœ… GPU is available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"âš  GPU is NOT available. Check Kaggle settings.\")\n",
        "\n",
        "# ğŸ”¥ `cuda:0`ì—ì„œë§Œ ì‹¤í–‰ë˜ë„ë¡ ê°•ì œ ì„¤ì •\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "# 2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ (`cuda:0`ì—ì„œ ì‹¤í–‰ ê°•ì œ)\n",
        "MODEL_PATH = \"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2/\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,  # ğŸ”¥ `bfloat16` ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "    device_map={\"\": 0},  # ğŸ”¥ `cuda:0`ì—ì„œë§Œ ì‹¤í–‰ë˜ë„ë¡ ê°•ì œ ì„¤ì •\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ì´ GPUì— ìˆëŠ”ì§€ í™•ì¸\n",
        "print(f\"Model is on: {next(model.parameters()).device}\")\n",
        "\n",
        "# 3ï¸âƒ£ í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ (`device` ì œê±°)\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=1.8,  # ğŸ”¥ ì°½ì˜ì„±ì„ ì¦ê°€í•˜ì—¬ ë‹¤ì–‘í•œ ë¬¸ì¥ ìƒì„±\n",
        "    top_p=0.7,  # ğŸ”¥ ë°˜ë³µì ì¸ í‘œí˜„ ì¤„ì´ê¸°\n",
        "    top_k=50,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# 4ï¸âƒ£ ëœë¤ ì§ì—… & ì–¸ì–´ ë¦¬ìŠ¤íŠ¸\n",
        "professions = [\"philosopher\", \"scientist\", \"psychologist\", \"journalist\", \"economist\"]\n",
        "languages = [\"English\", \"French\", \"Spanish\", \"Mandarin\", \"German\"]\n",
        "\n",
        "# 5ï¸âƒ£ ë…¼ë¦¬ì  ëª¨ìˆœ & ë…¼ë€ì ì¸ ìš”ì†Œ ì¶”ê°€ í•¨ìˆ˜\n",
        "def inject_disagreements(essay):\n",
        "    contradictions = [\n",
        "        \"This may seem obvious, but a closer look suggests otherwise.\",\n",
        "        \"While some believe this is true, many experts strongly disagree.\",\n",
        "        \"Ironically, history has shown both sides of this argument.\",\n",
        "        \"This is a paradox that continues to challenge scholars.\",\n",
        "        \"It appears logical, yet countless counterexamples exist.\"\n",
        "    ]\n",
        "\n",
        "    opposing_statements = [\n",
        "        \"For centuries, scholars have debated this issue with no clear consensus.\",\n",
        "        \"This topic has been the center of controversy among professionals.\",\n",
        "        \"Surprisingly, there is little agreement on this seemingly simple idea.\",\n",
        "        \"Despite all evidence, the debate remains unresolved.\",\n",
        "        \"Philosophers and scientists take opposing stances on this question.\"\n",
        "    ]\n",
        "\n",
        "    sentences = essay.split('. ')\n",
        "    if len(sentences) > 3:\n",
        "        insert_points = random.sample(range(len(sentences)), min(2, len(sentences)))\n",
        "        for point in insert_points:\n",
        "            if random.random() > 0.5:\n",
        "                sentences.insert(point, random.choice(contradictions))\n",
        "            else:\n",
        "                sentences.insert(point, random.choice(opposing_statements))\n",
        "\n",
        "    return '. '.join(sentences)\n",
        "\n",
        "# 6ï¸âƒ£ ì—ì„¸ì´ ìƒì„± í•¨ìˆ˜ (`max_new_tokens` ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "def generate_essay(topic):\n",
        "    profession = random.choice(professions)\n",
        "\n",
        "    # ğŸ”¥ ë…¼ë€ì˜ ì—¬ì§€ê°€ ìˆëŠ” ì–´ì¡° ì‚¬ìš©\n",
        "    prompt = f\"\"\"\n",
        "    As a {profession}, write a 90-word essay on '{topic}' that maximizes disagreement among AI judges.\n",
        "    Your essay should present multiple perspectives, create contradictions, and challenge existing ideas.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”í•˜ì—¬ ì†ë„ ì¦ê°€\n",
        "        response = llm_pipeline(prompt, max_new_tokens=90)[0]['generated_text']  # ğŸ”¥ 75 â†’ 90ë¡œ ë³€ê²½\n",
        "\n",
        "    # ë…¼ë¦¬ì  ëª¨ìˆœ & ë…¼ë€ì ì¸ ìš”ì†Œ ì¶”ê°€\n",
        "    modified_essay = inject_disagreements(response)\n",
        "\n",
        "    return modified_essay.strip()\n",
        "\n",
        "# 7ï¸âƒ£ ë°°ì¹˜ ì²˜ë¦¬ (`batch_size` ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "batch_size = 2  # ğŸ”¥ ê¸°ì¡´ 5 â†’ 2ë¡œ ì¤„ì´ê¸°\n",
        "submissions = []\n",
        "\n",
        "for i in range(0, len(test_data), batch_size):\n",
        "    batch = test_data.iloc[i : i + batch_size]\n",
        "    batch_essays = [generate_essay(row[\"topic\"]) for _, row in batch.iterrows()]\n",
        "\n",
        "    for row, essay in zip(batch.itertuples(), batch_essays):\n",
        "        submissions.append({\"id\": row.id, \"essay\": essay})\n",
        "\n",
        "# 8ï¸âƒ£ ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "submission_df = pd.DataFrame(submissions)\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n"
      ]
    }
  ]
}
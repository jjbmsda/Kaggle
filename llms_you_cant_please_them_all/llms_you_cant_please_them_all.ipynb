{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNldLYlZZXWkpgRh7XbRJb9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/Kaggle/blob/main/llms_you_cant_please_them_all/llms_you_cant_please_them_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "podKIpzaznHi"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 설치 및 로드\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# 1️⃣ Kaggle 데이터 로드\n",
        "test_data = pd.read_csv(\"/kaggle/input/llms-you-cant-please-them-all/test.csv\")\n",
        "\n",
        "# 2️⃣ 사용할 LLM 모델 설정 (모델 변경 가능)\n",
        "MODEL_PATH = \"/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1\"  # Phi-3 사용 가능\n",
        "# MODEL_PATH = \"/kaggle/input/gemma/transformers/2b-it/3/\"  # Gemma 사용 가능\n",
        "# MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/14b-instruct/1/\"  # Qwen 사용 가능\n",
        "\n",
        "# 모델 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# LLM 텍스트 생성 파이프라인 설정\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=1.5,  # 랜덤성 증가\n",
        "    top_p=0.9,\n",
        "    top_k=50,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# 3️⃣ 랜덤한 에세이 프롬프트 생성 (직업과 언어 랜덤 설정)\n",
        "professions = [\"philosopher\", \"scientist\", \"psychologist\", \"journalist\", \"economist\"]\n",
        "languages = [\"English\", \"French\", \"Spanish\", \"Mandarin\", \"German\"]\n",
        "\n",
        "BASE_PROMPT_TEMPLATE = \"\"\"Write a persuasive and thought-provoking essay on \"{topic}\" designed to elicit *strongly opposing reactions* from different AI judges.\n",
        "Some judges will analyze it from a scientific viewpoint, while others will assess it through philosophical and economic lenses.\n",
        "Your challenge is to create an essay that is controversial yet intellectually stimulating.\n",
        "Adopt the perspective of a {profession}, write in English, and incorporate cultural elements from the {language}-speaking world.\"\"\"\n",
        "\n",
        "# 4️⃣ 논리적 모순 삽입 함수 (AI 평가자를 혼란스럽게 하기 위함)\n",
        "def inject_contradictions(essay):\n",
        "    contradictions = [\n",
        "        \"While this may seem true, some experts argue the exact opposite.\",\n",
        "        \"Ironically, this idea has been both proven and disproven over time.\",\n",
        "        \"Despite all evidence supporting this, many continue to believe the contrary.\",\n",
        "        \"This conclusion appears valid, yet a deeper look suggests otherwise.\",\n",
        "        \"Although compelling, this argument is inherently self-contradictory.\"\n",
        "    ]\n",
        "    sentences = essay.split('. ')\n",
        "    insert_points = random.sample(range(len(sentences)), min(2, len(sentences)))\n",
        "\n",
        "    for point in insert_points:\n",
        "        contradiction = random.choice(contradictions)\n",
        "        sentences.insert(point, contradiction)\n",
        "\n",
        "    return '. '.join(sentences)\n",
        "\n",
        "# 5️⃣ LLM을 활용한 에세이 생성 함수\n",
        "def generate_essay(topic):\n",
        "    profession = random.choice(professions)\n",
        "    language = random.choice(languages)\n",
        "\n",
        "    prompt = BASE_PROMPT_TEMPLATE.format(topic=topic, profession=profession, language=language)\n",
        "\n",
        "    # LLM에 에세이 생성 요청\n",
        "    response = llm_pipeline(prompt, max_new_tokens=180)[0]['generated_text']\n",
        "\n",
        "    # 논리적 모순 추가\n",
        "    modified_essay = inject_contradictions(response)\n",
        "\n",
        "    return modified_essay.strip()\n",
        "\n",
        "# 6️⃣ 테스트 데이터에 대해 에세이 생성\n",
        "submissions = []\n",
        "for _, row in test_data.iterrows():\n",
        "    essay = generate_essay(row[\"topic\"])\n",
        "    submissions.append({\"id\": row[\"id\"], \"essay\": essay})\n",
        "\n",
        "# 7️⃣ 제출 파일 저장\n",
        "submission_df = pd.DataFrame(submissions)\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "# 8️⃣ 제출 파일 확인 (첫 3개 출력)\n",
        "submission_df.head()\n"
      ]
    }
  ]
}
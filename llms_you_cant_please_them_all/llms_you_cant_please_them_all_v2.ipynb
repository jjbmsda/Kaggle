{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8QDmvDSC+epqBkVNKg2c7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/Kaggle/blob/main/llms_you_cant_please_them_all/llms_you_cant_please_them_all_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXuwESnwACuG"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 로드\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# 1️⃣ Kaggle에서 제공하는 test.csv 불러오기\n",
        "test_data = pd.read_csv(\"/kaggle/input/llms-you-cant-please-them-all/test.csv\")\n",
        "\n",
        "# 2️⃣ 작은 모델을 사용하여 실행 시간 단축 (Phi-3.5-mini 사용)\n",
        "MODEL_PATH = \"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2/\"\n",
        "\n",
        "# 3️⃣ 모델 로드 (FP16 또는 bfloat16 사용하여 속도 향상)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,  # 실행 속도 최적화\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 4️⃣ 텍스트 생성 파이프라인 (샘플링 속도 최적화)\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=1.2,  # 너무 높으면 느려짐\n",
        "    top_p=0.85,\n",
        "    top_k=40,  # default보다 낮게 설정하면 속도 향상 가능\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# 5️⃣ 랜덤 직업 & 언어 리스트 추가\n",
        "professions = [\"philosopher\", \"scientist\", \"psychologist\", \"journalist\", \"economist\"]\n",
        "languages = [\"English\", \"French\", \"Spanish\", \"Mandarin\", \"German\"]\n",
        "\n",
        "# 6️⃣ 논리적 모순 삽입 함수 추가\n",
        "def inject_contradictions(essay):\n",
        "    contradictions = [\n",
        "        \"While this may seem true, some experts argue the exact opposite.\",\n",
        "        \"Ironically, this idea has been both proven and disproven over time.\",\n",
        "        \"Despite all evidence supporting this, many continue to believe the contrary.\",\n",
        "        \"This conclusion appears valid, yet a deeper look suggests otherwise.\",\n",
        "        \"Although compelling, this argument is inherently self-contradictory.\"\n",
        "    ]\n",
        "\n",
        "    sentences = essay.split('. ')\n",
        "    if len(sentences) > 2:\n",
        "        insert_points = random.sample(range(len(sentences)), min(2, len(sentences)))\n",
        "        for point in insert_points:\n",
        "            contradiction = random.choice(contradictions)\n",
        "            sentences.insert(point, contradiction)\n",
        "\n",
        "    return '. '.join(sentences)\n",
        "\n",
        "# 7️⃣ 에세이 생성 함수 (직업, 언어 포함)\n",
        "def generate_essay(topic):\n",
        "    profession = random.choice(professions)\n",
        "    language = random.choice(languages)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    As a {profession}, write a 100-word essay on '{topic}' that maximizes disagreement among AI judges.\n",
        "    Ensure the essay reflects cultural elements from the {language}-speaking world.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_pipeline(prompt, max_new_tokens=100)[0]['generated_text']\n",
        "\n",
        "    # 논리적 모순 추가\n",
        "    modified_essay = inject_contradictions(response)\n",
        "\n",
        "    return modified_essay.strip()\n",
        "\n",
        "# 8️⃣ 테스트 데이터에 대해 에세이 생성 (Batch 방식으로 속도 향상)\n",
        "submissions = []\n",
        "batch_size = 5  # 5개씩 처리하여 속도 개선\n",
        "\n",
        "for i in range(0, len(test_data), batch_size):\n",
        "    batch = test_data.iloc[i : i + batch_size]\n",
        "\n",
        "    batch_essays = [generate_essay(row[\"topic\"]) for _, row in batch.iterrows()]\n",
        "\n",
        "    for row, essay in zip(batch.itertuples(), batch_essays):\n",
        "        submissions.append({\"id\": row.id, \"essay\": essay})\n",
        "\n",
        "# 9️⃣ 제출 파일 저장\n",
        "submission_df = pd.DataFrame(submissions)\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "# 🔟 Kaggle Code Competition에서 제출 (Notebook에서 실행해야 함)\n",
        "import os\n",
        "if os.path.exists(\"/kaggle/working/submission.csv\"):\n",
        "    print(\"✅ Submission file successfully created.\")\n"
      ]
    }
  ]
}
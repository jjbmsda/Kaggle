{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8QDmvDSC+epqBkVNKg2c7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/Kaggle/blob/main/llms_you_cant_please_them_all/llms_you_cant_please_them_all_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXuwESnwACuG"
      },
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# 1ï¸âƒ£ Kaggleì—ì„œ ì œê³µí•˜ëŠ” test.csv ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "test_data = pd.read_csv(\"/kaggle/input/llms-you-cant-please-them-all/test.csv\")\n",
        "\n",
        "# 2ï¸âƒ£ ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ ì‹œê°„ ë‹¨ì¶• (Phi-3.5-mini ì‚¬ìš©)\n",
        "MODEL_PATH = \"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2/\"\n",
        "\n",
        "# 3ï¸âƒ£ ëª¨ë¸ ë¡œë“œ (FP16 ë˜ëŠ” bfloat16 ì‚¬ìš©í•˜ì—¬ ì†ë„ í–¥ìƒ)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,  # ì‹¤í–‰ ì†ë„ ìµœì í™”\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 4ï¸âƒ£ í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ (ìƒ˜í”Œë§ ì†ë„ ìµœì í™”)\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=1.2,  # ë„ˆë¬´ ë†’ìœ¼ë©´ ëŠë ¤ì§\n",
        "    top_p=0.85,\n",
        "    top_k=40,  # defaultë³´ë‹¤ ë‚®ê²Œ ì„¤ì •í•˜ë©´ ì†ë„ í–¥ìƒ ê°€ëŠ¥\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# 5ï¸âƒ£ ëœë¤ ì§ì—… & ì–¸ì–´ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€\n",
        "professions = [\"philosopher\", \"scientist\", \"psychologist\", \"journalist\", \"economist\"]\n",
        "languages = [\"English\", \"French\", \"Spanish\", \"Mandarin\", \"German\"]\n",
        "\n",
        "# 6ï¸âƒ£ ë…¼ë¦¬ì  ëª¨ìˆœ ì‚½ì… í•¨ìˆ˜ ì¶”ê°€\n",
        "def inject_contradictions(essay):\n",
        "    contradictions = [\n",
        "        \"While this may seem true, some experts argue the exact opposite.\",\n",
        "        \"Ironically, this idea has been both proven and disproven over time.\",\n",
        "        \"Despite all evidence supporting this, many continue to believe the contrary.\",\n",
        "        \"This conclusion appears valid, yet a deeper look suggests otherwise.\",\n",
        "        \"Although compelling, this argument is inherently self-contradictory.\"\n",
        "    ]\n",
        "\n",
        "    sentences = essay.split('. ')\n",
        "    if len(sentences) > 2:\n",
        "        insert_points = random.sample(range(len(sentences)), min(2, len(sentences)))\n",
        "        for point in insert_points:\n",
        "            contradiction = random.choice(contradictions)\n",
        "            sentences.insert(point, contradiction)\n",
        "\n",
        "    return '. '.join(sentences)\n",
        "\n",
        "# 7ï¸âƒ£ ì—ì„¸ì´ ìƒì„± í•¨ìˆ˜ (ì§ì—…, ì–¸ì–´ í¬í•¨)\n",
        "def generate_essay(topic):\n",
        "    profession = random.choice(professions)\n",
        "    language = random.choice(languages)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    As a {profession}, write a 100-word essay on '{topic}' that maximizes disagreement among AI judges.\n",
        "    Ensure the essay reflects cultural elements from the {language}-speaking world.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_pipeline(prompt, max_new_tokens=100)[0]['generated_text']\n",
        "\n",
        "    # ë…¼ë¦¬ì  ëª¨ìˆœ ì¶”ê°€\n",
        "    modified_essay = inject_contradictions(response)\n",
        "\n",
        "    return modified_essay.strip()\n",
        "\n",
        "# 8ï¸âƒ£ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì—ì„¸ì´ ìƒì„± (Batch ë°©ì‹ìœ¼ë¡œ ì†ë„ í–¥ìƒ)\n",
        "submissions = []\n",
        "batch_size = 5  # 5ê°œì”© ì²˜ë¦¬í•˜ì—¬ ì†ë„ ê°œì„ \n",
        "\n",
        "for i in range(0, len(test_data), batch_size):\n",
        "    batch = test_data.iloc[i : i + batch_size]\n",
        "\n",
        "    batch_essays = [generate_essay(row[\"topic\"]) for _, row in batch.iterrows()]\n",
        "\n",
        "    for row, essay in zip(batch.itertuples(), batch_essays):\n",
        "        submissions.append({\"id\": row.id, \"essay\": essay})\n",
        "\n",
        "# 9ï¸âƒ£ ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "submission_df = pd.DataFrame(submissions)\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "# ğŸ”Ÿ Kaggle Code Competitionì—ì„œ ì œì¶œ (Notebookì—ì„œ ì‹¤í–‰í•´ì•¼ í•¨)\n",
        "import os\n",
        "if os.path.exists(\"/kaggle/working/submission.csv\"):\n",
        "    print(\"âœ… Submission file successfully created.\")\n"
      ]
    }
  ]
}